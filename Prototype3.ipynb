{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to transform our labels data set into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each label file in the labels folder\n",
    "def create_csv_annotations(images_folder, labels_folder, name):\n",
    "    annotations = []\n",
    "    image_width = 300\n",
    "    image_height = 300\n",
    "\n",
    "    for label_file in os.listdir(labels_folder):\n",
    "        if label_file.endswith('.txt'):\n",
    "            with open(os.path.join(labels_folder, label_file), 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            \n",
    "            image_name = os.path.splitext(label_file)[0] + '.jpg'\n",
    "            image_path = os.path.join(images_folder, image_name)\n",
    "            \n",
    "            for line in lines:\n",
    "                class_label, x_center, y_center, width, height = map(float, line.split())\n",
    "                x_min = (x_center - width / 2)\n",
    "                y_min = (y_center - height / 2)\n",
    "                x_max = (x_center + width / 2)\n",
    "                y_max = (y_center + height / 2)\n",
    "                \n",
    "                annotations.append([image_path, x_min, y_min, x_max, y_max, image_width, image_height, class_label])\n",
    "\n",
    "        # Here we create a DataFrame from annotations list and then we convert the df into a csv file\n",
    "        df = pd.DataFrame(annotations, columns=['img_path', 'xmin', 'ymin', 'xmax', 'ymax', 'width', 'height', 'label'])\n",
    "        df.to_csv(name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_annotations('images/train', 'labels/train', 'annotations_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_annotations('images/val', 'labels/val', 'annotations_val.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Tensor flow intro -> Why sequential](https://towardsdatascience.com/a-comprehensive-introduction-to-tensorflows-sequential-api-and-model-for-deep-learning-c5e31aee49fa#:~:text=The%20sequential%20model%20allows%20us,for%20building%20deep%20learning%20models.)\n",
    "\n",
    "[Input and output shapes for CNN](https://towardsdatascience.com/understanding-input-and-output-shapes-in-convolution-network-keras-f143923d56ca)\n",
    "\n",
    "[Basics of the R-CNN model](https://towardsdatascience.com/object-detection-explained-r-cnn-a6c813937a76)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "19/19 [==============================] - 65s 3s/step - loss: 2.0125 - accuracy: 0.4150 - val_loss: 1.2283 - val_accuracy: 0.5400\n",
      "Epoch 2/10\n",
      "19/19 [==============================] - 50s 2s/step - loss: 1.0136 - accuracy: 0.5950 - val_loss: 0.9937 - val_accuracy: 0.6000\n",
      "Epoch 3/10\n",
      "19/19 [==============================] - 49s 3s/step - loss: 0.7612 - accuracy: 0.6950 - val_loss: 0.8840 - val_accuracy: 0.6133\n",
      "Epoch 4/10\n",
      "19/19 [==============================] - 47s 2s/step - loss: 0.6801 - accuracy: 0.7067 - val_loss: 0.8871 - val_accuracy: 0.6400\n",
      "Epoch 5/10\n",
      "19/19 [==============================] - 49s 3s/step - loss: 0.5860 - accuracy: 0.7483 - val_loss: 1.0470 - val_accuracy: 0.6533\n",
      "Epoch 6/10\n",
      "19/19 [==============================] - 48s 3s/step - loss: 0.4928 - accuracy: 0.7817 - val_loss: 0.8386 - val_accuracy: 0.6800\n",
      "Epoch 7/10\n",
      "19/19 [==============================] - 50s 3s/step - loss: 0.4780 - accuracy: 0.8100 - val_loss: 0.9211 - val_accuracy: 0.6800\n",
      "Epoch 8/10\n",
      "19/19 [==============================] - 50s 3s/step - loss: 0.5079 - accuracy: 0.7950 - val_loss: 1.0208 - val_accuracy: 0.6533\n",
      "Epoch 9/10\n",
      "19/19 [==============================] - 48s 3s/step - loss: 0.4277 - accuracy: 0.8000 - val_loss: 0.9029 - val_accuracy: 0.6733\n",
      "Epoch 10/10\n",
      "19/19 [==============================] - 45s 2s/step - loss: 0.3785 - accuracy: 0.8200 - val_loss: 1.0346 - val_accuracy: 0.6533\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1f233511c70>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load annotations from CSV\n",
    "train_annotations = pd.read_csv('annotations_train.csv')\n",
    "val_annotations = pd.read_csv('annotations_val.csv')\n",
    "\n",
    "#train_annotations, val_annotations = train_test_split(annotations, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "input_shape = (300, 300, 3)  # height, width, depth (this is the # of color channels RGB = 3)\n",
    "\n",
    "# Function to preprocess image and annotations -> this is because \n",
    "# the annotations are still not in the format required for TF\n",
    "def preprocess_data(annotation):\n",
    "    image = load_img(annotation['img_path'], target_size=(input_shape[0], input_shape[1]))\n",
    "    image_array = img_to_array(image)\n",
    "    image_array /= 255.0\n",
    "    bbox = [annotation['xmin'], annotation['ymin'], annotation['xmax'], annotation['ymax']]\n",
    "    label = annotation['label']\n",
    "    return image_array, bbox, label\n",
    "\n",
    "train_data = train_annotations.apply(preprocess_data, axis=1)\n",
    "val_data = val_annotations.apply(preprocess_data, axis=1)\n",
    "\n",
    "# Convert preprocessed data into arrays -> this is the format needed for TF\n",
    "X_train, y_train_bbox, y_train_label = zip(*train_data)\n",
    "X_val, y_val_bbox, y_val_label = zip(*val_data)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X_train = tf.convert_to_tensor(X_train)\n",
    "y_train_bbox = tf.convert_to_tensor(y_train_bbox)\n",
    "y_train_label = tf.convert_to_tensor(y_train_label)\n",
    "X_val = tf.convert_to_tensor(X_val)\n",
    "y_val_bbox = tf.convert_to_tensor(y_val_bbox)\n",
    "y_val_label = tf.convert_to_tensor(y_val_label)\n",
    "\n",
    "# This is the RCNN model, this is just base model for testing\n",
    "def create_rcnn_model(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),#batch_input_shape (if you wanted to give the batch_size)\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.Flatten(), # Here we are basically changing the 4D output of the CNN to 2D so that we can use Dense\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax', name='classifier_output')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Number of classes\n",
    "num_classes = train_annotations['label'].nunique()\n",
    "\n",
    "# Create an instance of the R-CNN model\n",
    "rcnn_model = create_rcnn_model(input_shape, num_classes)\n",
    "\n",
    "# Compile the model with appropriate losses and metrics\n",
    "rcnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "rcnn_model.fit(X_train, y_train_label, validation_data=(X_val, y_val_label), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 2s 366ms/step\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "     Vehiculos       0.00      0.00      0.00         5\n",
      "Construcciones       0.42      0.38      0.40        29\n",
      "          Vias       0.60      0.62      0.61        24\n",
      "          Rios       0.91      0.89      0.90        76\n",
      "       Mineria       0.17      0.25      0.20        16\n",
      "\n",
      "      accuracy                           0.65       150\n",
      "     macro avg       0.42      0.43      0.42       150\n",
      "  weighted avg       0.65      0.65      0.65       150\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 0  4  0  0  1]\n",
      " [ 0 11  3  0 15]\n",
      " [ 0  2 15  6  1]\n",
      " [ 0  0  5 68  3]\n",
      " [ 0  9  2  1  4]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\juank\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\juank\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\juank\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred = np.argmax(rcnn_model.predict(X_val), axis=1)\n",
    "\n",
    "y_val_label = np.array(y_val_label, dtype=int)\n",
    "y_pred = np.array(y_pred, dtype=int)\n",
    "\n",
    "target_names = ['Vehiculos', 'Construcciones', 'Vias', 'Rios', 'Mineria']  # Get unique class labels\n",
    "print(classification_report(y_val_label, y_pred, target_names=target_names))\n",
    "\n",
    "conf_matrix = confusion_matrix(y_val_label, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3.0' '2.0' '1.0' '4.0' '0.0']\n"
     ]
    }
   ],
   "source": [
    "print(target_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
